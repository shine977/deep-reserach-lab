/**
 * Reasoning Plugin
 *
 * This plugin processes information using LLM reasoning.
 * It integrates with language models to analyze and reason about content.
 */

import { Observable } from "rxjs";
import {
  NodePlugin,
  PluginContext,
  ExecutionContext,
  JSONSchema,
} from "../../core/types/plugin.types";
import axios from "axios";
import { Injectable } from "@deep-research-lab/core/di";

export interface ReasoningResult {
  reasoning: string;
  answer: string;
  confidence: number;
  nextAction?: string;
}

@Injectable()
export class ReasoningPlugin implements NodePlugin {
  // Plugin metadata
  id = "reasoning-plugin";
  name = "LLM Reasoning";
  version = "1.0.0";
  description = "Performs reasoning using language models";
  nodeType = "reasoning";

  // Schema definitions
  inputSchema: JSONSchema = {
    type: "object",
    properties: {
      contents: {
        type: "array",
        items: {
          type: "object",
          properties: {
            url: { type: "string" },
            title: { type: "string" },
            content: { type: "string" },
          },
        },
      },
      query: { type: "string" },
      context: { type: "string" },
    },
  };

  outputSchema: JSONSchema = {
    type: "object",
    properties: {
      reasoning: { type: "string" },
      answer: { type: "string" },
      confidence: { type: "number" },
      nextAction: { type: "string" },
    },
  };

  configSchema: JSONSchema = {
    type: "object",
    properties: {
      model: {
        type: "string",
        enum: ["mock", "gemini", "openai"],
        default: "mock",
      },
      temperature: {
        type: "number",
        default: 0.7,
      },
      useMock: {
        type: "boolean",
        default: true,
      },
    },
  };

  // Plugin lifecycle methods
  async initialize(context: PluginContext): Promise<void> {
    context.logger.info("Initializing reasoning plugin");
  }

  async activate(): Promise<void> {
    console.log("Reasoning plugin activated");
  }

  async deactivate(): Promise<void> {
    console.log("Reasoning plugin deactivated");
  }

  // Process method
  process(input: any, config: any, context: ExecutionContext): Observable<any> {
    const query = input.query || "No query provided";
    const contents = input.contents || [];
    const additionalContext = input.context || "";

    context.logger.info(
      `Processing reasoning with ${contents.length} content items`,
    );

    const model = config.model || "mock";
    const temperature = config.temperature || 0.7;
    const useMock = config.useMock !== undefined ? config.useMock : true;

    // Build prompt
    const prompt = this.buildPrompt(query, contents, additionalContext);

    // Process with selected model
    if (useMock || model === "mock") {
      return this.mockReasoning(prompt, context);
    } else if (model === "gemini") {
      return this.reasonWithGemini(prompt, temperature, context);
    } else if (model === "openai") {
      return this.reasonWithOpenAI(prompt, temperature, context);
    } else {
      return this.mockReasoning(prompt, context);
    }
  }

  // Build prompt from input
  private buildPrompt(
    query: string,
    contents: any[],
    additionalContext: string,
  ): string {
    let prompt = `Question: ${query}\n\n`;

    if (additionalContext) {
      prompt += `Context: ${additionalContext}\n\n`;
    }

    prompt += "Information from sources:\n\n";

    contents.forEach((item, index) => {
      prompt += `Source ${index + 1} [${item.url}]: ${item.title}\n`;
      prompt += `${item.content.substring(0, 500)}${item.content.length > 500 ? "..." : ""}\n\n`;
    });

    prompt += `Based on the information above, please answer the question: ${query}\n`;
    prompt += "Your response should be structured as follows:\n";
    prompt += "1. Reasoning: Your step-by-step reasoning process\n";
    prompt += "2. Answer: Your final answer to the question\n";
    prompt +=
      "3. Confidence: A number between 0 and 1 indicating your confidence\n";
    prompt +=
      "4. Next Action: What to do next (search, visit, answer, or reflect)\n";

    return prompt;
  }

  // Mock reasoning for testing
  private mockReasoning(
    prompt: string,
    context: ExecutionContext,
  ): Observable<ReasoningResult> {
    return new Observable((observer) => {
      setTimeout(() => {
        // Extract the query from the prompt
        const queryMatch = prompt.match(/Question: (.*?)(?:\n|$)/);
        const query = queryMatch ? queryMatch[1] : "unknown query";

        const result: ReasoningResult = {
          reasoning: `This is mock reasoning for the query: "${query}". In a real system, this would be generated by an LLM.
          
Step 1: Analyzed the available information from the provided sources.
Step 2: Identified key points relevant to the query.
Step 3: Synthesized the information to formulate a comprehensive answer.`,
          answer: `This is a mock answer to "${query}". In production, this would be an actual answer generated by an LLM based on the provided information.`,
          confidence: 0.85,
          nextAction: ["search", "visit", "answer", "reflect"][
            Math.floor(Math.random() * 4)
          ],
        };

        observer.next(result);
        observer.complete();
      }, 1000); // Simulate LLM processing time
    });
  }

  // Reason with Gemini
  private reasonWithGemini(
    prompt: string,
    temperature: number,
    context: ExecutionContext,
  ): Observable<ReasoningResult> {
    return new Observable((observer) => {
      // Check for API key
      const geminiApiKey = process.env.GEMINI_API_KEY;
      if (!geminiApiKey) {
        observer.error(new Error("GEMINI_API_KEY is not set"));
        return;
      }

      // API endpoint
      const apiUrl =
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent";

      // Request payload
      const payload = {
        contents: [
          {
            parts: [{ text: prompt }],
          },
        ],
        generationConfig: {
          temperature: temperature,
          maxOutputTokens: 2048,
        },
      };

      // Make API request
      axios
        .post(`${apiUrl}?key=${geminiApiKey}`, payload)
        .then((response) => {
          try {
            const text = response.data.candidates[0].content.parts[0].text;

            // Parse the response
            const reasoningMatch = /Reasoning:(.*?)(?:Answer:|$)/s.exec(text);
            const answerMatch = /Answer:(.*?)(?:Confidence:|$)/s.exec(text);
            const confidenceMatch = /Confidence:(.*?)(?:Next Action:|$)/s.exec(
              text,
            );
            const nextActionMatch = /Next Action:(.*?)(?:$)/s.exec(text);

            const result: ReasoningResult = {
              reasoning: reasoningMatch
                ? reasoningMatch[1].trim()
                : "No reasoning provided",
              answer: answerMatch
                ? answerMatch[1].trim()
                : "No answer provided",
              confidence: confidenceMatch
                ? parseFloat(confidenceMatch[1].trim()) || 0.5
                : 0.5,
              nextAction: nextActionMatch
                ? nextActionMatch[1].trim().toLowerCase()
                : "reflect",
            };

            observer.next(result);
            observer.complete();
          } catch (error) {
            context.logger.error("Error parsing Gemini response:", error);
            observer.error(error);
          }
        })
        .catch((error) => {
          context.logger.error("Gemini API error:", error);
          observer.error(error);
        });
    });
  }

  // Reason with OpenAI
  private reasonWithOpenAI(
    prompt: string,
    temperature: number,
    context: ExecutionContext,
  ): Observable<ReasoningResult> {
    return new Observable((observer) => {
      // Check for API key
      const openaiApiKey = process.env.OPENAI_API_KEY;
      if (!openaiApiKey) {
        observer.error(new Error("OPENAI_API_KEY is not set"));
        return;
      }

      // API endpoint
      const apiUrl = "https://api.openai.com/v1/chat/completions";

      // Request payload
      const payload = {
        model: "gpt-4",
        messages: [
          {
            role: "user",
            content: prompt,
          },
        ],
        temperature: temperature,
        max_tokens: 2048,
      };

      // Make API request
      axios
        .post(apiUrl, payload, {
          headers: {
            Authorization: `Bearer ${openaiApiKey}`,
            "Content-Type": "application/json",
          },
        })
        .then((response) => {
          try {
            const text = response.data.choices[0].message.content;

            // Parse the response
            const reasoningMatch = /Reasoning:(.*?)(?:Answer:|$)/s.exec(text);
            const answerMatch = /Answer:(.*?)(?:Confidence:|$)/s.exec(text);
            const confidenceMatch = /Confidence:(.*?)(?:Next Action:|$)/s.exec(
              text,
            );
            const nextActionMatch = /Next Action:(.*?)(?:$)/s.exec(text);

            const result: ReasoningResult = {
              reasoning: reasoningMatch
                ? reasoningMatch[1].trim()
                : "No reasoning provided",
              answer: answerMatch
                ? answerMatch[1].trim()
                : "No answer provided",
              confidence: confidenceMatch
                ? parseFloat(confidenceMatch[1].trim()) || 0.5
                : 0.5,
              nextAction: nextActionMatch
                ? nextActionMatch[1].trim().toLowerCase()
                : "reflect",
            };

            observer.next(result);
            observer.complete();
          } catch (error) {
            context.logger.error("Error parsing OpenAI response:", error);
            observer.error(error);
          }
        })
        .catch((error) => {
          context.logger.error("OpenAI API error:", error);
          observer.error(error);
        });
    });
  }
}
